{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import rpy2 as R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleaning and Imputation\n",
    "\n",
    "# load data\n",
    "# put the data file in the same directory as this notebook and replace the empty string with the name of the data file\n",
    "Hackathon_data_path = \"\"\n",
    "# change this to read_file_type depending on the file type (e.g. pd.read_csv, pd.read_excel, etc.)\n",
    "hackathon_data_uncleaned = pd.read_csv(Hackathon_data_path)\n",
    "# open the file in data wrangler to view the data\n",
    "\n",
    "# clean the data here (e.g. drop columns, rename columns, etc.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cleaned_hackathon_data = hackathon_data_uncleaned.copy()  # replace this with the cleaned data\n",
    "\n",
    "# label the features and target variable\n",
    "X = cleaned_hackathon_data.drop(columns=[\"target_column\"])  # replace \"target_column\" with the name of the target variable\n",
    "y = cleaned_hackathon_data[\"target_column\"]  # replace \"target_column\" with the name of the target variable\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8080e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imputation methods\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "\n",
    "# IterativeImputer is a more advanced imputation method that models each feature with missing values as a function of other features in a round-robin fashion. \n",
    "# It can be more accurate than simpler methods like mean or median imputation, especially when the data has complex relationships between features.\n",
    "# It's important to note that IterativeImputer can be computationally intensive, especially for large datasets, and it may not always be the best choice depending on the specific characteristics of your data and the amount of missingness.\n",
    "# If you have a large dataset or a high percentage of missing values, you may want to consider simpler imputation methods or dimensionality reduction techniques before applying IterativeImputer.\n",
    "# Additionally, it's crucial to evaluate the performance of the imputation method you choose, as it can significantly impact the results of your analysis or machine learning models.\n",
    "# It's also a good practice to compare the results of different imputation methods to ensure that the chosen method is appropriate for your specific dataset and analysis goals.\n",
    "# The autofill wrote all of this ^ wow.\n",
    "# I was just going to day that IterativeImputer is like MICE in R\n",
    "\n",
    "iterative_imputer = IterativeImputer(random_state=0)\n",
    "simple_imputer = SimpleImputer(strategy=\"mean\")  # replace \"mean\" with the desired imputation strategy (e.g. \"median\", \"most_frequent\", etc.)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)  # replace 5 with the desired number of neighbors for KNN imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines!\n",
    "# create a pipeline that includes the imputer and any other preprocessing steps (e.g. scaling, encoding, etc.)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Pipelines\n",
    "pipeline_steps = [\n",
    "    (\"imputer\", iterative_imputer),  # replace iterative_imputer with the desired imputation method (e.g. simple_imputer, knn_imputer, etc.)\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),  # replace OneHotEncoder with the desired encoding method (e.g. OrdinalEncoder, etc.)\n",
    "    (\"column_transformer\", ColumnTransformer(transformers=[(\"power_transformer\", PowerTransformer(method=\"yeo-johnson\"), [\"col1\", \"col2\"])], remainder=\"passthrough\")),  # replace 'col1', 'col2' with the names of the columns to be transformed, and add more tuples to the list if you have more transformations to apply to different columns. The remainder='passthrough' argument ensures that any columns not specified in the transformers list are passed through without transformation.\n",
    "    (\"model\", None)  # replace None with the desired machine learning model (e.g. LogisticRegression(), RandomForestClassifier(), etc.)\n",
    "]\n",
    "\n",
    "# note on scaling: These models are sensitive to the scale of the features, and scaling can help improve their performance and convergence.\n",
    "# Gradient Descent Models: Neural Networks (MLP) and Logistic Regression converge much faster when features are on a similar scale.\n",
    "# Regularized Models: If you use Lasso or Ridge Regression, scaling is required because these methods penalize coefficients based on their magnitude.\n",
    "# Dimensionality Reduction: PCA requires scaling because it seeks to maximize variance; unscaled data will lead PCA to focus only on the features with the largest raw values.\n",
    "\n",
    "# note on encoding: OneHotEncoder is a common choice for encoding categorical variables, especially when the categories are nominal (i.e. no inherent order). \n",
    "# It creates binary columns for each category, which can be useful for many machine learning algorithms. \n",
    "# However, if you have a large number of categories, it can lead to a high-dimensional feature space, which may require additional dimensionality reduction techniques or regularization to prevent overfitting.\n",
    "# If your categorical variables are ordinal (i.e. have a natural order), you may want to use OrdinalEncoder instead, which assigns integer values to the categories based on their order. \n",
    "# However, be cautious when using OrdinalEncoder with algorithms that can interpret the integer values as having a meaningful order, as this may not always be appropriate.\n",
    "\n",
    "pipeline = Pipeline(steps=pipeline_steps)\n",
    "pipeline.fit(X_train,y_train)  # fit the pipeline to the training data\n",
    "pipeline.score(X_test, y_test)  # evaluate the pipeline on the test data\n",
    "pipeline.predict(X_test)  # make predictions using the pipeline on the test data\n",
    "pipeline.named_steps[\"model\"].feature_importances_  # access feature importances from the model in the pipeline (replace \"model\" with the name of the model step in the pipeline, e.g. \"random_forest\", etc.)\n",
    "pipeline.named_steps[\"model\"].coef_  # access model coefficients from the model in the pipeline (replace \"model\" with the name of the model step in the pipeline, e.g. \"logistic_regression\", etc.)\n",
    "\n",
    "\n",
    "# transformations\n",
    "# power transformation is a technique used to stabilize variance and make the data more normally distributed. It can be particularly useful for skewed data. The PowerTransformer in scikit-learn provides two methods for power transformation: \"yeo-johnson\" and \"box-cox\". The \"yeo-johnson\" method can handle both positive and negative values, while the \"box-cox\" method can only handle positive values. You can choose the appropriate method based on the characteristics of your data.\n",
    "yeo_johnson_transformer = PowerTransformer(method=\"yeo-johnson\")  # replace \"yeo-johnson\" with the desired transformation method (e.g. \"box-cox\", etc.)\n",
    "# ct = ColumnTranformer([(\"yeo_johnson\", yeo_johnson_transformer), ['col1', 'col2'])], remainder='passthrough')  # replace 'col1', 'col2' with the names of the columns to be transformed, and add more tuples to the list if you have more transformations to apply to different columns. The remainder='passthrough' argument ensures that any columns not specified in the transformers list are passed through without transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Hyperparameter Tuning if we want to\n",
    "model = None  # replace None with the desired machine learning model (e.g. LogisticRegression(), RandomForestClassifier(), etc.)\n",
    "\n",
    "param_grid = {\n",
    "    # Tune the model (step name is \"model\")\n",
    "    'model': [model(random_state=42)], # Set the model here if it was None\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    \n",
    "    # You can even tune the imputer or transformer!\n",
    "    'imputer__initial_strategy': ['mean', 'median'],\n",
    "    'column_transformer__power_transformer__method': ['yeo-johnson']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,            # 5-fold cross-validation\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1        # Use all available CPU cores\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "# Predict\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Access the model inside the best pipeline to get importances\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "importances = best_pipeline.named_steps[\"model\"].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# evaluate the model using appropriate metrics (e.g. classification_report, confusion_matrix, etc.)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))  # replace with the appropriate metric for your specific problem (e.g. regression metrics, etc.)\n",
    "print(confusion_matrix(y_test, y_pred))  # replace with the appropriate metric for your specific problem (e.g. regression metrics, etc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
